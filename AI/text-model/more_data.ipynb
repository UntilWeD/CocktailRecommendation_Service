{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# Okt 형태소 분석기 초기화\n",
    "okt = Okt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:   0%|          | 0/13 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 100%|██████████| 13/13 [00:00<00:00, 161.06it/s]\n"
     ]
    }
   ],
   "source": [
    "# Okt 형태소 분석기 초기화\n",
    "okt = Okt()\n",
    "\n",
    "def process_batch(texts, batch_size=32):\n",
    "    results = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i + batch_size]\n",
    "        # 배치 단위로 형태소 분석\n",
    "        morphs_batch = [okt.pos(text, norm=True, stem=True) for text in batch]\n",
    "        results.extend(morphs_batch)\n",
    "    return results\n",
    "\n",
    "def augment_korean_text(text, num_augments=2):\n",
    "    augmented_texts = set()\n",
    "    morphs = okt.pos(text, norm=True, stem=True)\n",
    "    \n",
    "    for _ in range(num_augments * 2):  # 원하는 개수보다 더 많이 시도\n",
    "        if len(augmented_texts) >= num_augments:\n",
    "            break\n",
    "            \n",
    "        new_text = []\n",
    "        changed = False\n",
    "        \n",
    "        for word, pos in morphs:\n",
    "            if pos in ['Adjective', 'Verb'] and word in synonyms:\n",
    "                if random.random() < 0.7:\n",
    "                    new_word = random.choice(synonyms[word])\n",
    "                    new_text.append(new_word)\n",
    "                    changed = True\n",
    "                else:\n",
    "                    new_text.append(word)\n",
    "            else:\n",
    "                new_text.append(word)\n",
    "        \n",
    "        new_sentence = ' '.join(new_text)\n",
    "        if changed and new_sentence != text:\n",
    "            augmented_texts.add(new_sentence)\n",
    "    \n",
    "    return list(augmented_texts)\n",
    "\n",
    "def process_chunk(chunk):\n",
    "    results = []\n",
    "    for _, row in chunk.iterrows():\n",
    "        augmented_texts = augment_korean_text(row[\"입력 문장\"])\n",
    "        for aug_text in augmented_texts:\n",
    "            results.append({\n",
    "                \"입력 문장\": aug_text,\n",
    "                \"도수\": row[\"도수\"],\n",
    "                \"술 종류\": row[\"술 종류\"],\n",
    "                \"맛\": row[\"맛\"]\n",
    "            })\n",
    "    return results\n",
    "\n",
    "# 메인 실행 코드\n",
    "if __name__ == \"__main__\":\n",
    "    # 데이터 로드\n",
    "    df = pd.read_csv(\"data/raw_data.csv\", encoding='utf-8')\n",
    "    \n",
    "    # CPU 코어 수에 따라 청크 분할\n",
    "    num_cores = multiprocessing.cpu_count() - 1\n",
    "    chunk_size = max(1, len(df) // num_cores)\n",
    "    chunks = [df[i:i + chunk_size] for i in range(0, len(df), chunk_size)]\n",
    "    \n",
    "    augmented_data = []\n",
    "    \n",
    "    # 병렬 처리\n",
    "    with ThreadPoolExecutor(max_workers=num_cores) as executor:\n",
    "        futures = [executor.submit(process_chunk, chunk) for chunk in chunks]\n",
    "        \n",
    "        # tqdm으로 진행률 표시\n",
    "        for future in tqdm(futures, total=len(chunks), desc=\"Processing chunks\"):\n",
    "            augmented_data.extend(future.result())\n",
    "    \n",
    "    # 원본 데이터도 포함\n",
    "    original_data = df.to_dict('records')\n",
    "    augmented_data.extend(original_data)\n",
    "    \n",
    "    # 결과를 DataFrame으로 변환 및 저장\n",
    "    final_df = pd.DataFrame(augmented_data)\n",
    "    final_df.to_csv(\"data/augmented_data.csv\", index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
